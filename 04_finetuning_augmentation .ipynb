{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточные сети для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Skip-connections (2 балла)\n",
    "\n",
    "Постройте архитектуру свёрточной сети, аналогичную архитектуре в примере ниже, но добавьте в неё skip-connections, то есть дополнительные рёбра в вычислительном графе, позволяющие пропускать градиент в более ранние слои напрямую, минуя очередной блок Conv2D + BatchNorm + ReLU:\n",
    "\n",
    "```python\n",
    "def forward(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.block1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = x + self.block2(x)\n",
    "    x = self.maxpool(x)\n",
    "    ...\n",
    "    x = x.adaptive_maxpool(x).flatten(1)\n",
    "    logits = self.fc(x)\n",
    "    return logits\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша верхнеуровневая архитектура будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[nn.Module],\n",
    "        n_classes: int,\n",
    "        hidden_channels: list[int] = [32, 64],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # входной слой, принимающий изображение с 3-мя каналами\n",
    "        self.in_conv = nn.Conv2d(3, hidden_channels[0], kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # собираем свёрточные блоки, каждый задаётся кол-вом входных и выходных каналов\n",
    "        blocks = []\n",
    "        for c_in, c_out in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            # добавляем очередной блок\n",
    "            blocks.append(block(c_in, c_out))\n",
    "            # добавляем Max pooling для уменьшения размерности\n",
    "            blocks.append(nn.MaxPool2d(2, 2))\n",
    "\n",
    "        # собираем блоки в единый Sequential модуль для удобства\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # линейный слой для классификации\n",
    "        self.fc = nn.Linear(hidden_channels[-1], n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.features(self.relu(self.in_conv(x)))\n",
    "        logits = self.fc(self.maxpool(h).flatten(1))\n",
    "        return logits\n",
    "\n",
    "#m = MyResNet(block=BasicBlock, n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый блок, без residual connections, состоит из двух свёрток и нормализаций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # first conv + bn + nonlinearity\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        # second conv + bn\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # final nonlinearity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результат его применения к тензору:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно изменить этот блок, добавив в него skip-connection. Теперь в методе `forward` входной тензор `x` пойдёт по двум веткам:\n",
    "1. как в базовом блоке, через наши всёртки и нормализации, до последней нелинейности\n",
    "2. в обход свёрток и нормализаций\n",
    "\n",
    "В конце эти ветки нужно объединить через сумму. Тут есть проблема: в исходном тензоре `x` и обработанном нашим блоком `h(x)` отличается количество каналов (остальные размерности совпадают). То есть нам нужно сравнять количество каналов исходного тензора `inplanes` с количеством выходных каналов `outplanes`.\n",
    "\n",
    "Интуитивно, если рассматривать каждый пиксел входного тензора как вектор размера `inplanes`, в вектор размера `planes` его можно превратить домножением на матрицу размера `inplanes x planes`. Это можно сделать, создав свёрточный слой с размером кернела 1 - он и будет переводить наши пикселы в другую размерность.\n",
    "\n",
    "Не забудьте к сумме каналов применить нелинейность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # добавьте свёртку 1x1 для изменения кол-ва каналов входного тензора\n",
    "        self.conv1d = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # сохраним входной тензор на будущее\n",
    "        # ВАШ ХОД\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.relu(out)\n",
    "        out = out + self.conv1d(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим размеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ResidualBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape == torch.Size(\n",
    "    [3, 6, 32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что модель выдаёт тензор ожидаемого размера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 128]).forward(\n",
    "    torch.randn(3, 3, 32, 32)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем создавать модели разного размера, в том числе достаточно большие и глубокие, чтобы хорошо классифицировать изображения из датасета CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151047"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    p.numel()\n",
    "    for p in MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 64]).parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2. Обучение `MyResNet` с использованием Lightning (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача: добиться 80% точности на валидационной выборке с вашей реализацией `MyResNet`.\n",
    "\n",
    "После окончания обучения используйте метод `Trainer.validate` для вывода ваших метрик с удачного чекпоинта модели.\n",
    "\n",
    "NB: вызывайте `Trainer.validate` везде, где в задании требуется достичь какой-то точности\n",
    "\n",
    "\n",
    "Советы:\n",
    "- По умолчанию Lightning сохраняет только последний чекпоинт, так что вам может потребоваться `lightning.callbacks.ModelCheckpoint`, чтобы сохранять лучший чекпоинт в процессе обучения.\n",
    "\n",
    "- Используйте tensorboard, чтобы следить за динамикой обучения. Если заметите переобучение - подключайте регуляризацию. Большая модель с регуляризацией обычно лучше маленькой модели без неё.\n",
    "\n",
    "- Чтобы добиться нужной точности, ваша модель должна быть достаточно глубокой, ориентируйтесь на 4-5 блоков. Если необходимо, подключайте регуляризацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модуль данных с аугументациями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''transform_to = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            #transforms.RandomHorizontalFlip(), \n",
    "\n",
    "    \n",
    "])'''\n",
    "\n",
    "class Datamodule1(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor]=transforms.ToTensor(),\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # в этом методе можно сделать предварительную работу, например\n",
    "        # скачать данные, сделать тяжёлый препроцессинг\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # аргумент `stage` будет приходить из модуля обучения Trainer\n",
    "        # на стадии обучения (fit) нам нужны оба датасета\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            ), datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([transforms.RandomHorizontalFlip(), self.transform]),\n",
    "            )])\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        # на стадии валидации (validate) - только тестовый\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "datamodule1 = Datamodule1(batch_size=32, num_workers=0)\n",
    "datamodule1.setup(stage=\"fit\")\n",
    "batch1 = next(iter(datamodule1.train_dataloader()))\n",
    "for i in batch1:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс для организации обучения и добавим метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import torchmetrics.classification\n",
    "\n",
    "\n",
    "def create_classification_metrics(\n",
    "    num_classes: int, prefix: str\n",
    ") -> torchmetrics.MetricCollection:\n",
    "    return torchmetrics.MetricCollection(\n",
    "        [\n",
    "            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "            #torchmetrics.classification.MulticlassAUROC(\n",
    "            #    num_classes=num_classes, average=\"macro\"\n",
    "            #),\n",
    "        ],\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "\n",
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_metrics = create_classification_metrics(\n",
    "            num_classes=10, prefix=\"train_\"\n",
    "        )\n",
    "        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT: \n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # loss теперь сохраняем только раз в эпоху\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.train_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.val_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n",
    "       \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": y_hat,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.learning_rate, weight_decay=0.00001)\n",
    "        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=[5, 10, 15]\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name                             | Type               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model                            | MyResNet           | 615 K  | train\n",
      "1 | model.in_conv                    | Conv2d             | 448    | train\n",
      "2 | model.relu                       | ReLU               | 0      | train\n",
      "3 | model.features                   | Sequential         | 613 K  | train\n",
      "4 | model.maxpool                    | AdaptiveMaxPool2d  | 0      | train\n",
      "5 | model.fc                         | Linear             | 1.3 K  | train\n",
      "6 | train_metrics                    | MetricCollection   | 0      | train\n",
      "7 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n",
      "8 | val_metrics                      | MetricCollection   | 0      | train\n",
      "9 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.462     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.model_summary import summarize\n",
    "\n",
    "my_res_net = Lit(\n",
    "    model=MyResNet(ResidualBlock, n_classes = 10, hidden_channels=[16, 32, 64, 128, 128]), learning_rate=0.001\n",
    ")\n",
    "print(summarize(my_res_net, max_depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим callbacks для вывода потери:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torchmetrics.classification.confusion_matrix import ConfusionMatrix\n",
    "\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: L.Trainer, pl_module: L.LightningModule\n",
    "    ) -> None:\n",
    "        print(f'Accuracy val: {my_res_net.val_metrics.compute()}')\n",
    "\n",
    "\n",
    "callbacks = [MyPrintingCallback()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем trainer и запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | MyResNet         | 615 K  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.462     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6ccbcad0044165a8daf4097d40b0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.0938, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf4d495c30f4ee1bb4077e180d9b5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73d5954ab5540a4adfc57acb1c35ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.5475, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e89c315d245a38c2c0d4374003a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.5792, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb10f02b55c4614962cd30923f5a5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.6477, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faf2120519c464dbb43ca9a9e71033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.6884, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee159bf5c377443fa483abeae0b594a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7396, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b479040c1148f99e1bdb29a5d59fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7861, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fff813a1e2044b2b64d25a71b0cba9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7968, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec099978ddb74a96b7f90b5b5d048036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7993, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26aee074d41e495587f60100cb1f51e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8014, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10e5f9b34104c6f9027266a6482806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8066, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d92e65589d48dd8a910bdcaf3fe40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8075, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83b207688a94b5fa3f07fe3ddc753b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8083, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de2c1a18de94305986a4c3aae914a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8080, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff341ee5976e4faeabfdba02b91a36ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8079, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f7286a7e19412892732bbc3f3dcbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8097, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from aim.pytorch_lightning import AimLogger\n",
    "# import os\n",
    "\n",
    "# Set the environment variable\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "u = \"Задание 2\"\n",
    "logger2 = AimLogger(repo=\"logs\", experiment=str(u))\n",
    "\n",
    "trainer2 = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=15,\n",
    "    limit_train_batches=500,\n",
    "    limit_val_batches=500,\n",
    "    logger=logger2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "trainer2.fit(\n",
    "    model=my_res_net,\n",
    "    datamodule=datamodule1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3. Добавление аугментаций (1 балл + 2 балла за точность на валидации более 85%)\n",
    "# (я добавил из сразу в модули данных)\n",
    "\n",
    "Добавьте к обучающему датасету аугментации - случайные трансформации входных данных. Для этого можно использовать `torchvision.transforms` и `albumentations`.\n",
    "\n",
    "С `torchvision.transforms` совсем просто: вам нужно будет при создании `Datamodule` из практики по `lightning` указать вместо\n",
    "\n",
    "```python\n",
    "transform = transforms.ToTensor()\n",
    "```\n",
    "композицию трансформаций:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # случайное зеркальное отражение\n",
    "    ...\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете `albumentations` аугментаций значительно больше:\n",
    "\n",
    "![albumentations](https://albumentations.ai/assets/img/custom/top_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4. Использование предобученной модели (4 балла)\n",
    "\n",
    "Теперь мы научимся использовать модели, обученные на других задачах\n",
    "\n",
    "Ваша задача: добиться 90% точности на тестовой выборке CIFAR-10. Постарайтесь уложиться модель с ~5 млн параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `torchvision.models` есть много реализованных архитектур, размером которых можно удобно управлять. Например, ниже можно создать крошечную версию модели `MobileNetV2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2(\n",
    "    num_classes=10,\n",
    "    width_mult=0.4,\n",
    "    inverted_residual_setting=[\n",
    "        # t, c, n, s\n",
    "        [1, 16, 1, 1],\n",
    "        [3, 24, 2, 2],\n",
    "        [3, 32, 3, 2],\n",
    "    ],\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "sum([param.numel() for param in mobilenet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но кроме архитектуры модели, мы также можем скачать веса, полученные при обучении на каком-то датасете. Например, для нашей задачи можно использовать предобучение на самом известном датасете для классификации изображений - ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5288548"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0\n",
    "\n",
    "# создаём EfficientNet с весами, полученными на ImageNet\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "efficientnet = efficientnet_b0(weights=weights)\n",
    "sum([param.numel() for param in efficientnet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 1.** С использованием модели в исходном виде есть проблема: в ImageNet 1000 классов, а у нас только 10. Поэтому в предобученной модели нужно будет полностью заменить последний линейный слой, который даёт распределение вероятностей классов. Это можно сделать уже в готовом объекте модели, переназначив атрибут.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` линейный слой находится в атрибуте `classifier` \n",
    "\n",
    "\n",
    "**Указание 2.** Все слои, кроме нескольких последних (может быть, только последнего) мы можем заморозить, то есть сделать значения параметров в них неизменными. Это позволит и сохранить способность модели выделять полезные низкоуровневые признаки (она научилась этому на ImageNet), и существенно ускорить дообучение.\n",
    "\n",
    "\n",
    "Чтобы заморозить параметры, нужно всего лишь отключить для них расчёт градиентов. Вернитесь к первой практике, чтобы вспомнить, как это можно сделать. Нам подойдёт самый простой способ с `.requires_grad`.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` свёрточные слои находятся в атрибуте `features` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 3.** Предобученные модели на ImageNet ожидают специальным образом трансформированные изображения:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому эти трансформации нужно будет передать в датамодуль (как мы делали с аугментациями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВАШ ХОД: Обучите модель и выведите результат метода validate на удачном чекпоинте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Модуль данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''transform_to = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            #transforms.RandomHorizontalFlip(), \n",
    "\n",
    "    \n",
    "])'''\n",
    "\n",
    "transform_to = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "class Datamodule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor] = transform_to,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # в этом методе можно сделать предварительную работу, например\n",
    "        # скачать данные, сделать тяжёлый препроцессинг\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # аргумент `stage` будет приходить из модуля обучения Trainer\n",
    "        # на стадии обучения (fit) нам нужны оба датасета\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            ), datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([self.transform,self.transform]),\n",
    "            )])\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transform_to,\n",
    "            )\n",
    "        # на стадии валидации (validate) - только тестовый\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transform_to,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "datamodule = Datamodule(batch_size=32, num_workers=0)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "for item in batch:\n",
    "    print(item.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменяем последний слой в архитектуре используемой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dNormActivation(\n",
       "  (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True),\n",
    "                                        nn.Linear(in_features=1280, out_features=10, bias=True))\n",
    "efficientnet.features[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключаем расчет градиентов во всех слоях кроме последнего:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                                        nn.Conv2d(320, 320, kernel_size=3, stride=1, padding=1, bias=False),\\n                                        nn.BatchNorm2d(320),\\n                                        nn.ReLU(),\\n    \\n                                        nn.Conv2d(320, 320, kernel_size=3, stride=1, padding=1, bias=False),\\n                                        nn.BatchNorm2d(320),\\n                                        nn.ReLU(),\\n    \\n                                        nn.Conv2d(320, 1280, kernel_size=(1,1), stride=(1,1), bias=False),\\n                                        nn.BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\\n                                        nn.SiLU(inplace=True)\\n                                        )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(efficientnet.features) - 3):\n",
    "    for param in efficientnet.features[i].parameters():\n",
    "       param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import torchmetrics.classification\n",
    "\n",
    "\n",
    "def create_classification_metrics(\n",
    "    num_classes: int, prefix: str\n",
    ") -> torchmetrics.MetricCollection:\n",
    "    return torchmetrics.MetricCollection(\n",
    "        [\n",
    "            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        ],\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "\n",
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_metrics = create_classification_metrics(\n",
    "            num_classes=10, prefix=\"train_\"\n",
    "        )\n",
    "        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT: \n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # loss теперь сохраняем только раз в эпоху\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.train_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.val_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n",
    "       \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": y_hat,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=[5, 10, 15]\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name                             | Type               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model                            | EfficientNet       | 4.0 M  | train\n",
      "1 | model.features                   | Sequential         | 4.0 M  | train\n",
      "2 | model.avgpool                    | AdaptiveAvgPool2d  | 0      | train\n",
      "3 | model.classifier                 | Sequential         | 12.8 K | train\n",
      "4 | train_metrics                    | MetricCollection   | 0      | train\n",
      "5 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n",
      "6 | val_metrics                      | MetricCollection   | 0      | train\n",
      "7 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "851 K     Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.081    Total estimated model params size (MB)\n",
      "341       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.model_summary import summarize\n",
    "\n",
    "lit_module = Lit(\n",
    "    model=efficientnet, learning_rate=0.001\n",
    ")\n",
    "print(summarize(lit_module, max_depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим callbacks для вывода потери:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torchmetrics.classification.confusion_matrix import ConfusionMatrix\n",
    "\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: L.Trainer, pl_module: L.LightningModule\n",
    "    ) -> None:\n",
    "        print(f'Accuracy val: {lit_module.val_metrics.compute()}')\n",
    "\n",
    "\n",
    "callbacks = [MyPrintingCallback()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем trainer и запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | EfficientNet     | 4.0 M  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "851 K     Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.081    Total estimated model params size (MB)\n",
      "341       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3b8772dfea439d8faacfe64d125958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.1094, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede7ce4a72964f58a8679b0da8a1cb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3f121b9dc4efaa8210122eab33cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7453, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4bb22c82b4d45ab271c679a4b8339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7862, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a2b6c1d714f0e9810f59ea2175ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8019, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2992c392064e5b8066450bfde69b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8487, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694fdb782b8f4d5fb9b90143c67c5d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8272, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4d0a039e584834baa9c3b3b0bf7d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8703, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b52d96d38b48b98fecab0523a84892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8888, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5742a9a270c4d4696c601b652a22369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8925, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2339ab18ddf463aa85697f6dfd9b0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8975, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc52a5563424422eb96440e832ec044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.9025, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from aim.pytorch_lightning import AimLogger\n",
    "# import os\n",
    "\n",
    "# Set the environment variable\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "u = \"Задание 4\"\n",
    "logger = AimLogger(repo=\"logs\", experiment=str(u))\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=10,\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=100,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
