{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточные сети для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Skip-connections (2 балла)\n",
    "\n",
    "Постройте архитектуру свёрточной сети, аналогичную архитектуре в примере ниже, но добавьте в неё skip-connections, то есть дополнительные рёбра в вычислительном графе, позволяющие пропускать градиент в более ранние слои напрямую, минуя очередной блок Conv2D + BatchNorm + ReLU:\n",
    "\n",
    "```python\n",
    "def forward(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.block1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = x + self.block2(x)\n",
    "    x = self.maxpool(x)\n",
    "    ...\n",
    "    x = x.adaptive_maxpool(x).flatten(1)\n",
    "    logits = self.fc(x)\n",
    "    return logits\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша верхнеуровневая архитектура будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[nn.Module],\n",
    "        n_classes: int,\n",
    "        hidden_channels: list[int] = [32, 64],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # входной слой, принимающий изображение с 3-мя каналами\n",
    "        self.in_conv = nn.Conv2d(3, hidden_channels[0], kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # собираем свёрточные блоки, каждый задаётся кол-вом входных и выходных каналов\n",
    "        blocks = []\n",
    "        for c_in, c_out in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            # добавляем очередной блок\n",
    "            blocks.append(block(c_in, c_out))\n",
    "            # добавляем Max pooling для уменьшения размерности\n",
    "            blocks.append(nn.MaxPool2d(2, 2))\n",
    "\n",
    "        # собираем блоки в единый Sequential модуль для удобства\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # линейный слой для классификации\n",
    "        self.fc = nn.Linear(hidden_channels[-1], n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.features(self.relu(self.in_conv(x)))\n",
    "        logits = self.fc(self.maxpool(h).flatten(1))\n",
    "        return logits\n",
    "\n",
    "#m = MyResNet(block=BasicBlock, n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый блок, без residual connections, состоит из двух свёрток и нормализаций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # first conv + bn + nonlinearity\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        # second conv + bn\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # final nonlinearity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результат его применения к тензору:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно изменить этот блок, добавив в него skip-connection. Теперь в методе `forward` входной тензор `x` пойдёт по двум веткам:\n",
    "1. как в базовом блоке, через наши всёртки и нормализации, до последней нелинейности\n",
    "2. в обход свёрток и нормализаций\n",
    "\n",
    "В конце эти ветки нужно объединить через сумму. Тут есть проблема: в исходном тензоре `x` и обработанном нашим блоком `h(x)` отличается количество каналов (остальные размерности совпадают). То есть нам нужно сравнять количество каналов исходного тензора `inplanes` с количеством выходных каналов `outplanes`.\n",
    "\n",
    "Интуитивно, если рассматривать каждый пиксел входного тензора как вектор размера `inplanes`, в вектор размера `planes` его можно превратить домножением на матрицу размера `inplanes x planes`. Это можно сделать, создав свёрточный слой с размером кернела 1 - он и будет переводить наши пикселы в другую размерность.\n",
    "\n",
    "Не забудьте к сумме каналов применить нелинейность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # добавьте свёртку 1x1 для изменения кол-ва каналов входного тензора\n",
    "        self.conv1d = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # сохраним входной тензор на будущее\n",
    "        # ВАШ ХОД\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.relu(out)\n",
    "        out = out + self.conv1d(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим размеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ResidualBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape == torch.Size(\n",
    "    [3, 6, 32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что модель выдаёт тензор ожидаемого размера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 128]).forward(\n",
    "    torch.randn(3, 3, 32, 32)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем создавать модели разного размера, в том числе достаточно большие и глубокие, чтобы хорошо классифицировать изображения из датасета CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151047"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    p.numel()\n",
    "    for p in MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 64]).parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2. Обучение `MyResNet` с использованием Lightning (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача: добиться 80% точности на валидационной выборке с вашей реализацией `MyResNet`.\n",
    "\n",
    "После окончания обучения используйте метод `Trainer.validate` для вывода ваших метрик с удачного чекпоинта модели.\n",
    "\n",
    "NB: вызывайте `Trainer.validate` везде, где в задании требуется достичь какой-то точности\n",
    "\n",
    "\n",
    "Советы:\n",
    "- По умолчанию Lightning сохраняет только последний чекпоинт, так что вам может потребоваться `lightning.callbacks.ModelCheckpoint`, чтобы сохранять лучший чекпоинт в процессе обучения.\n",
    "\n",
    "- Используйте tensorboard, чтобы следить за динамикой обучения. Если заметите переобучение - подключайте регуляризацию. Большая модель с регуляризацией обычно лучше маленькой модели без неё.\n",
    "\n",
    "- Чтобы добиться нужной точности, ваша модель должна быть достаточно глубокой, ориентируйтесь на 4-5 блоков. Если необходимо, подключайте регуляризацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модуль данных с аугументациями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''transform_to = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            #transforms.RandomHorizontalFlip(), \n",
    "\n",
    "    \n",
    "])'''\n",
    "\n",
    "class Datamodule1(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor]=transforms.ToTensor(),\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # в этом методе можно сделать предварительную работу, например\n",
    "        # скачать данные, сделать тяжёлый препроцессинг\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # аргумент `stage` будет приходить из модуля обучения Trainer\n",
    "        # на стадии обучения (fit) нам нужны оба датасета\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            ), datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([transforms.RandomHorizontalFlip(), self.transform]),\n",
    "            )])\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        # на стадии валидации (validate) - только тестовый\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "datamodule1 = Datamodule1(batch_size=32, num_workers=0)\n",
    "datamodule1.setup(stage=\"fit\")\n",
    "batch1 = next(iter(datamodule1.train_dataloader()))\n",
    "for i in batch1:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс для организации обучения и добавим метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import torchmetrics.classification\n",
    "\n",
    "\n",
    "def create_classification_metrics(\n",
    "    num_classes: int, prefix: str\n",
    ") -> torchmetrics.MetricCollection:\n",
    "    return torchmetrics.MetricCollection(\n",
    "        [\n",
    "            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "            #torchmetrics.classification.MulticlassAUROC(\n",
    "            #    num_classes=num_classes, average=\"macro\"\n",
    "            #),\n",
    "        ],\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "\n",
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_metrics = create_classification_metrics(\n",
    "            num_classes=10, prefix=\"train_\"\n",
    "        )\n",
    "        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT: \n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # loss теперь сохраняем только раз в эпоху\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.train_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.val_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n",
    "       \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": y_hat,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.learning_rate, weight_decay=0.00001)\n",
    "        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=[5, 10, 15]\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name                             | Type               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model                            | MyResNet           | 615 K  | train\n",
      "1 | model.in_conv                    | Conv2d             | 448    | train\n",
      "2 | model.relu                       | ReLU               | 0      | train\n",
      "3 | model.features                   | Sequential         | 613 K  | train\n",
      "4 | model.maxpool                    | AdaptiveMaxPool2d  | 0      | train\n",
      "5 | model.fc                         | Linear             | 1.3 K  | train\n",
      "6 | train_metrics                    | MetricCollection   | 0      | train\n",
      "7 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n",
      "8 | val_metrics                      | MetricCollection   | 0      | train\n",
      "9 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.462     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.model_summary import summarize\n",
    "\n",
    "my_res_net = Lit(\n",
    "    model=MyResNet(ResidualBlock, n_classes = 10, hidden_channels=[16, 32, 64, 128, 128]), learning_rate=0.001\n",
    ")\n",
    "print(summarize(my_res_net, max_depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим callbacks для вывода потери:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torchmetrics.classification.confusion_matrix import ConfusionMatrix\n",
    "\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: L.Trainer, pl_module: L.LightningModule\n",
    "    ) -> None:\n",
    "        print(f'Accuracy val: {my_res_net.val_metrics.compute()}')\n",
    "\n",
    "\n",
    "callbacks = [MyPrintingCallback(),\n",
    "             ModelCheckpoint(\n",
    "                    filename=\"{epoch}-{val_loss:.2f}\",\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "        save_top_k=2,\n",
    "        save_last=True,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=3,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем trainer и запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | MyResNet         | 615 K  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.462     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277126c7c7d44954b78a6fa8531020de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.0781, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0234cd4c15f54d978bf392762fb901a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6977c3b076664b8ab40b86f7dd224a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.4648, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c0a06d2d8f40e48c2490bca9ead453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.6324, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287586576f341778177ab93267d875c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.6647, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7f081017ee4342b3f6b13e6f25e452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.6855, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb563476dfe4448aacd29ebfe514ee84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7321, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a61dc8381f43f292e0f0a27771b081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7917, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08ae4e5c8004036b155ebb5f4f72285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7985, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727d22dac8de490997212c31bb7de036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8036, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9323a8a626481e81276e4afd30eca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8062, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec0328e47fd4f85a3eee0f8067767c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8079, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb7246abf304628951995c5cb330474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8091, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f857b6e18f104b7593efea1470b20295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8088, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e4862523394639bcaf9672e8bf54bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8110, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87301f82be414440a7d01e0894bb679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8111, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0e070ac4204ef1bbf9fd97aa9a0d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8119, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from aim.pytorch_lightning import AimLogger\n",
    "# import os\n",
    "\n",
    "# Set the environment variable\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "u = \"Задание 2\"\n",
    "logger2 = AimLogger(repo=\"logs\", experiment=str(u))\n",
    "\n",
    "trainer2 = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=15,\n",
    "    limit_train_batches=500,\n",
    "    limit_val_batches=500,\n",
    "    logger=logger2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "trainer2.fit(\n",
    "    model=my_res_net,\n",
    "    datamodule=datamodule1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Force-releasing locks for Run 'd6b09a6a177046dda25edc0d'. Data corruption may occur if there is active process writing to Run 'd6b09a6a177046dda25edc0d'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26bd732431e44f3b4baa3fe94dbe005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8119, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  val_MulticlassAccuracy   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8119000196456909     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5457688570022583     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m val_MulticlassAccuracy  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8119000196456909    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5457688570022583    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.5457688570022583,\n",
       "  'val_MulticlassAccuracy': 0.8119000196456909}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.validate(\n",
    "    model=my_res_net,\n",
    "    datamodule=datamodule1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3. Добавление аугментаций (1 балл + 2 балла за точность на валидации более 85%)\n",
    "# (я добавил из сразу в модули данных)\n",
    "\n",
    "Добавьте к обучающему датасету аугментации - случайные трансформации входных данных. Для этого можно использовать `torchvision.transforms` и `albumentations`.\n",
    "\n",
    "С `torchvision.transforms` совсем просто: вам нужно будет при создании `Datamodule` из практики по `lightning` указать вместо\n",
    "\n",
    "```python\n",
    "transform = transforms.ToTensor()\n",
    "```\n",
    "композицию трансформаций:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # случайное зеркальное отражение\n",
    "    ...\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете `albumentations` аугментаций значительно больше:\n",
    "\n",
    "![albumentations](https://albumentations.ai/assets/img/custom/top_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4. Использование предобученной модели (4 балла)\n",
    "\n",
    "Теперь мы научимся использовать модели, обученные на других задачах\n",
    "\n",
    "Ваша задача: добиться 90% точности на тестовой выборке CIFAR-10. Постарайтесь уложиться модель с ~5 млн параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `torchvision.models` есть много реализованных архитектур, размером которых можно удобно управлять. Например, ниже можно создать крошечную версию модели `MobileNetV2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46322"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2(\n",
    "    num_classes=10,\n",
    "    width_mult=0.4,\n",
    "    inverted_residual_setting=[\n",
    "        # t, c, n, s\n",
    "        [1, 16, 1, 1],\n",
    "        [3, 24, 2, 2],\n",
    "        [3, 32, 3, 2],\n",
    "    ],\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "sum([param.numel() for param in mobilenet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но кроме архитектуры модели, мы также можем скачать веса, полученные при обучении на каком-то датасете. Например, для нашей задачи можно использовать предобучение на самом известном датасете для классификации изображений - ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5288548"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0\n",
    "\n",
    "# создаём EfficientNet с весами, полученными на ImageNet\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "efficientnet = efficientnet_b0(weights=weights)\n",
    "sum([param.numel() for param in efficientnet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 1.** С использованием модели в исходном виде есть проблема: в ImageNet 1000 классов, а у нас только 10. Поэтому в предобученной модели нужно будет полностью заменить последний линейный слой, который даёт распределение вероятностей классов. Это можно сделать уже в готовом объекте модели, переназначив атрибут.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` линейный слой находится в атрибуте `classifier` \n",
    "\n",
    "\n",
    "**Указание 2.** Все слои, кроме нескольких последних (может быть, только последнего) мы можем заморозить, то есть сделать значения параметров в них неизменными. Это позволит и сохранить способность модели выделять полезные низкоуровневые признаки (она научилась этому на ImageNet), и существенно ускорить дообучение.\n",
    "\n",
    "\n",
    "Чтобы заморозить параметры, нужно всего лишь отключить для них расчёт градиентов. Вернитесь к первой практике, чтобы вспомнить, как это можно сделать. Нам подойдёт самый простой способ с `.requires_grad`.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` свёрточные слои находятся в атрибуте `features` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 3.** Предобученные модели на ImageNet ожидают специальным образом трансформированные изображения:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому эти трансформации нужно будет передать в датамодуль (как мы делали с аугментациями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВАШ ХОД: Обучите модель и выведите результат метода validate на удачном чекпоинте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Модуль данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''transform_to = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            #transforms.RandomHorizontalFlip(), \n",
    "\n",
    "    \n",
    "])'''\n",
    "\n",
    "transform_to = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "class Datamodule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor] = transform_to,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # в этом методе можно сделать предварительную работу, например\n",
    "        # скачать данные, сделать тяжёлый препроцессинг\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # аргумент `stage` будет приходить из модуля обучения Trainer\n",
    "        # на стадии обучения (fit) нам нужны оба датасета\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            ), datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transforms.Compose([self.transform,self.transform]),\n",
    "            )])\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transform_to,\n",
    "            )\n",
    "        # на стадии валидации (validate) - только тестовый\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transform_to,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "datamodule = Datamodule(batch_size=32, num_workers=0)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "for item in batch:\n",
    "    print(item.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменяем последний слой в архитектуре используемой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dNormActivation(\n",
       "  (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True),\n",
    "                                        nn.Linear(in_features=1280, out_features=10, bias=True))\n",
    "efficientnet.features[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключаем расчет градиентов во всех слоях кроме последнего:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(efficientnet.features) - 3):\n",
    "    for param in efficientnet.features[i].parameters():\n",
    "       param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import torchmetrics.classification\n",
    "\n",
    "\n",
    "def create_classification_metrics(\n",
    "    num_classes: int, prefix: str\n",
    ") -> torchmetrics.MetricCollection:\n",
    "    return torchmetrics.MetricCollection(\n",
    "        [\n",
    "            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        ],\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "\n",
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_metrics = create_classification_metrics(\n",
    "            num_classes=10, prefix=\"train_\"\n",
    "        )\n",
    "        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT: \n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # loss теперь сохраняем только раз в эпоху\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.train_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # обновляем метрики и логируем раз в эпоху\n",
    "        self.val_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n",
    "       \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": y_hat,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=[5, 10, 15]\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name                             | Type               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model                            | EfficientNet       | 4.0 M  | train\n",
      "1 | model.features                   | Sequential         | 4.0 M  | train\n",
      "2 | model.avgpool                    | AdaptiveAvgPool2d  | 0      | train\n",
      "3 | model.classifier                 | Sequential         | 12.8 K | train\n",
      "4 | train_metrics                    | MetricCollection   | 0      | train\n",
      "5 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n",
      "6 | val_metrics                      | MetricCollection   | 0      | train\n",
      "7 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "851 K     Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.081    Total estimated model params size (MB)\n",
      "341       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.model_summary import summarize\n",
    "\n",
    "lit_module = Lit(\n",
    "    model=efficientnet, learning_rate=0.001\n",
    ")\n",
    "print(summarize(lit_module, max_depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим callbacks для вывода потери:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torchmetrics.classification.confusion_matrix import ConfusionMatrix\n",
    "\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: L.Trainer, pl_module: L.LightningModule\n",
    "    ) -> None:\n",
    "        print(f'Accuracy val: {lit_module.val_metrics.compute()}')\n",
    "\n",
    "\n",
    "callbacks = [MyPrintingCallback()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем trainer и запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | EfficientNet     | 4.0 M  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "851 K     Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.081    Total estimated model params size (MB)\n",
      "341       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22b69ac2f6b46578b66e65f2370f4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.0625, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae62f5b93d74689aa4408f86a1f63b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b16698441e4ffe8eef4621bcec97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.7500, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8729d1394ae43468f5d95e886a2f2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8153, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb461d40dd1e4058ade3fe8a855a5b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8313, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3ff85c090a46ef968332c897bb85cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8259, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c76c6c402304281b0dde610bafb2a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8547, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa832445797d423e93ed6849a6cdc238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8766, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e72e2a6f09d4e77a1c2b8508d0f3b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8838, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fa5d673ccd4270b4adf52f59c736b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8884, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76f3136cb143149136a9519c98480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8919, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4972a254924142d487ca0c34ac4a4848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8972, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34f256439834ac4814eea189b3cf56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8969, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e51d0519d874c6683105939cfb3e46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=12` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8972, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from aim.pytorch_lightning import AimLogger\n",
    "# import os\n",
    "\n",
    "# Set the environment variable\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "u = \"Задание 4\"\n",
    "logger = AimLogger(repo=\"logs\", experiment=str(u))\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=12,\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=100,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Force-releasing locks for Run '583f9ac0168b4435ba498b07'. Data corruption may occur if there is active process writing to Run '583f9ac0168b4435ba498b07'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea42763f7ba4ed4938699a8d9977eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: {'val_MulticlassAccuracy': tensor(0.8972, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  val_MulticlassAccuracy   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8971874713897705     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.29850125312805176    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m val_MulticlassAccuracy  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8971874713897705    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.29850125312805176   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.29850125312805176,\n",
       "  'val_MulticlassAccuracy': 0.8971874713897705}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
