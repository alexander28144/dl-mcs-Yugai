{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Свёрточные сети для классификации","metadata":{}},{"cell_type":"code","source":"from typing import Type\n\n\n\nimport torch\n\nfrom torch import Tensor, nn\n\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:49:40.942838Z","iopub.execute_input":"2024-10-11T00:49:40.943482Z","iopub.status.idle":"2024-10-11T00:49:44.102207Z","shell.execute_reply.started":"2024-10-11T00:49:40.943444Z","shell.execute_reply":"2024-10-11T00:49:44.101217Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#### Задание 1. Skip-connections (2 балла)\n\n\n\nПостройте архитектуру свёрточной сети, аналогичную архитектуре в примере ниже, но добавьте в неё skip-connections, то есть дополнительные рёбра в вычислительном графе, позволяющие пропускать градиент в более ранние слои напрямую, минуя очередной блок Conv2D + BatchNorm + ReLU:\n\n\n\n```python\n\ndef forward(self, x: Tensor) -> Tensor:\n\n    x = x + self.block1(x)\n\n    x = self.maxpool(x)\n\n    x = x + self.block2(x)\n\n    x = self.maxpool(x)\n\n    ...\n\n    x = x.adaptive_maxpool(x).flatten(1)\n\n    logits = self.fc(x)\n\n    return logits\n\n```\n","metadata":{}},{"cell_type":"markdown","source":"Наша верхнеуровневая архитектура будет выглядеть так:","metadata":{}},{"cell_type":"code","source":"class MyResNet(nn.Module):\n\n    def __init__(\n\n        self,\n\n        block: Type[nn.Module],\n\n        n_classes: int,\n\n        hidden_channels: list[int] = [32, 64],\n\n    ) -> None:\n\n        super().__init__()\n\n        # входной слой, принимающий изображение с 3-мя каналами\n\n        self.in_conv = nn.Conv2d(3, hidden_channels[0], kernel_size=3, stride=1)\n\n        self.relu = nn.ReLU(inplace=True)\n\n\n\n        # собираем свёрточные блоки, каждый задаётся кол-вом входных и выходных каналов\n\n        blocks = []\n\n        for c_in, c_out in zip(hidden_channels[:-1], hidden_channels[1:]):\n\n            # добавляем очередной блок\n\n            blocks.append(block(c_in, c_out))\n\n            # добавляем Max pooling для уменьшения размерности\n\n            blocks.append(nn.MaxPool2d(2, 2))\n\n\n\n        # собираем блоки в единый Sequential модуль для удобства\n\n        self.features = nn.Sequential(*blocks)\n\n        self.maxpool = nn.AdaptiveMaxPool2d(1)\n\n\n\n        # линейный слой для классификации\n\n        self.fc = nn.Linear(hidden_channels[-1], n_classes)\n\n\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        h = self.features(self.relu(self.in_conv(x)))\n\n        logits = self.fc(self.maxpool(h).flatten(1))\n\n        return logits\n\n\n\n#m = MyResNet(block=BasicBlock, n_classes=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:49:47.552891Z","iopub.execute_input":"2024-10-11T00:49:47.553382Z","iopub.status.idle":"2024-10-11T00:49:47.563694Z","shell.execute_reply.started":"2024-10-11T00:49:47.553344Z","shell.execute_reply":"2024-10-11T00:49:47.562812Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Базовый блок, без residual connections, состоит из двух свёрток и нормализаций:","metadata":{}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n\n    def __init__(self, inplanes: int, planes: int) -> None:\n\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n\n            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n\n        )\n\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(\n\n            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n\n        )\n\n        self.bn2 = nn.BatchNorm2d(planes)\n\n\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        # first conv + bn + nonlinearity\n\n        out = self.relu(self.bn1(self.conv1(x)))\n\n        # second conv + bn\n\n        out = self.bn2(self.conv2(out))\n\n        # final nonlinearity\n\n        out = self.relu(out)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:41:25.975667Z","iopub.execute_input":"2024-10-11T00:41:25.976001Z","iopub.status.idle":"2024-10-11T00:41:25.982705Z","shell.execute_reply.started":"2024-10-11T00:41:25.975973Z","shell.execute_reply":"2024-10-11T00:41:25.981919Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Посмотрим на результат его применения к тензору:","metadata":{}},{"cell_type":"code","source":"BasicBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 6, 32, 32])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"execution_count":4},{"cell_type":"markdown","source":"Теперь нужно изменить этот блок, добавив в него skip-connection. Теперь в методе `forward` входной тензор `x` пойдёт по двум веткам:\n\n1. как в базовом блоке, через наши всёртки и нормализации, до последней нелинейности\n\n2. в обход свёрток и нормализаций\n\n\n\nВ конце эти ветки нужно объединить через сумму. Тут есть проблема: в исходном тензоре `x` и обработанном нашим блоком `h(x)` отличается количество каналов (остальные размерности совпадают). То есть нам нужно сравнять количество каналов исходного тензора `inplanes` с количеством выходных каналов `outplanes`.\n\n\n\nИнтуитивно, если рассматривать каждый пиксел входного тензора как вектор размера `inplanes`, в вектор размера `planes` его можно превратить домножением на матрицу размера `inplanes x planes`. Это можно сделать, создав свёрточный слой с размером кернела 1 - он и будет переводить наши пикселы в другую размерность.\n\n\n\nНе забудьте к сумме каналов применить нелинейность.","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n\n    def __init__(self, inplanes: int, planes: int) -> None:\n\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n\n            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n\n        )\n\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(\n\n            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n\n        )\n\n        self.bn2 = nn.BatchNorm2d(planes)\n\n\n\n        # добавьте свёртку 1x1 для изменения кол-ва каналов входного тензора\n\n        self.conv1d = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n\n\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        # сохраним входной тензор на будущее\n\n        # ВАШ ХОД\n\n        identity = x\n\n        out = self.relu(self.bn1(self.conv1(x)))\n\n        out = self.bn2(self.conv2(out))\n\n        out = self.relu(out)\n\n        out = out + self.conv1d(x)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:41:30.740577Z","iopub.execute_input":"2024-10-11T00:41:30.741078Z","iopub.status.idle":"2024-10-11T00:41:30.747296Z","shell.execute_reply.started":"2024-10-11T00:41:30.741046Z","shell.execute_reply":"2024-10-11T00:41:30.746693Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Проверим размеры:","metadata":{}},{"cell_type":"code","source":"assert ResidualBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape == torch.Size(\n\n    [3, 6, 32, 32]\n\n)","metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Проверим, что модель выдаёт тензор ожидаемого размера:","metadata":{}},{"cell_type":"code","source":"MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 128]).forward(\n\n    torch.randn(3, 3, 32, 32)\n\n).shape","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 7])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"cell_type":"markdown","source":"Теперь мы можем создавать модели разного размера, в том числе достаточно большие и глубокие, чтобы хорошо классифицировать изображения из датасета CIFAR-10.","metadata":{}},{"cell_type":"code","source":"sum(\n\n    p.numel()\n\n    for p in MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 64]).parameters()\n\n)","metadata":{},"outputs":[{"data":{"text/plain":["151047"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"execution_count":8},{"cell_type":"markdown","source":"#### Задание 2. Обучение `MyResNet` с использованием Lightning (5 баллов)","metadata":{}},{"cell_type":"markdown","source":"Ваша задача: добиться 80% точности на валидационной выборке с вашей реализацией `MyResNet`.\n\n\n\nПосле окончания обучения используйте метод `Trainer.validate` для вывода ваших метрик с удачного чекпоинта модели.\n\n\n\nNB: вызывайте `Trainer.validate` везде, где в задании требуется достичь какой-то точности\n\n\n\n\n\nСоветы:\n\n- По умолчанию Lightning сохраняет только последний чекпоинт, так что вам может потребоваться `lightning.callbacks.ModelCheckpoint`, чтобы сохранять лучший чекпоинт в процессе обучения.\n\n\n\n- Используйте tensorboard, чтобы следить за динамикой обучения. Если заметите переобучение - подключайте регуляризацию. Большая модель с регуляризацией обычно лучше маленькой модели без неё.\n\n\n\n- Чтобы добиться нужной точности, ваша модель должна быть достаточно глубокой, ориентируйтесь на 4-5 блоков. Если необходимо, подключайте регуляризацию","metadata":{}},{"cell_type":"markdown","source":"#### Модуль данных с аугументациями:","metadata":{}},{"cell_type":"code","source":"from typing import Callable\n\n\n\nimport lightning as L\n\nfrom lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n\nfrom PIL.Image import Image\n\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import datasets, transforms\n\n\n\n'''transform_to = transforms.Compose([\n\n            transforms.CenterCrop(224),\n\n            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n\n            transforms.ToTensor(),\n\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n            #transforms.RandomHorizontalFlip(), \n\n\n\n    \n\n])'''\n\n\n\nclass Datamodule1(L.LightningDataModule):\n\n    def __init__(\n\n        self,\n\n        batch_size: int,\n\n        transform: Callable[[Image], Tensor]=transforms.ToTensor(),\n\n        num_workers: int = 0,\n\n    ):\n\n        super().__init__()\n\n        self.batch_size = batch_size\n\n        self.transform = transform\n\n        self.num_workers = num_workers\n\n\n\n    def prepare_data(self) -> None:\n\n        # в этом методе можно сделать предварительную работу, например\n\n        # скачать данные, сделать тяжёлый препроцессинг\n\n        pass\n\n\n\n    def setup(self, stage: str) -> None:\n\n        # аргумент `stage` будет приходить из модуля обучения Trainer\n\n        # на стадии обучения (fit) нам нужны оба датасета\n\n        if stage == \"fit\":\n\n            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n\n                \"data\",\n\n                train=True,\n\n                download=True,\n\n                transform=transforms.ToTensor(),\n\n            ), datasets.CIFAR10(\n\n                \"data\",\n\n                train=True,\n\n                download=True,\n\n                transform=transforms.Compose([transforms.RandomHorizontalFlip(), self.transform]),\n\n            )])\n\n            self.val_dataset = datasets.CIFAR10(\n\n                \"data\",\n\n                train=False,\n\n                download=True,\n\n                transform=transforms.ToTensor(),\n\n            )\n\n        # на стадии валидации (validate) - только тестовый\n\n        elif stage == \"validate\":\n\n            self.val_dataset = datasets.CIFAR10(\n\n                \"data\",\n\n                train=False,\n\n                download=True,\n\n                transform=transforms.ToTensor(),\n\n            )\n\n        else:\n\n            raise NotImplementedError\n\n        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n\n\n\n    def train_dataloader(self) -> TRAIN_DATALOADERS:\n\n        return DataLoader(\n\n            self.train_dataset,\n\n            batch_size=self.batch_size,\n\n            shuffle=True,\n\n            num_workers=self.num_workers,\n\n        )\n\n\n\n    def val_dataloader(self) -> EVAL_DATALOADERS:\n\n        return DataLoader(\n\n            self.val_dataset,\n\n            batch_size=self.batch_size,\n\n            shuffle=False,\n\n            num_workers=self.num_workers,\n\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:51:36.367056Z","iopub.execute_input":"2024-10-11T00:51:36.367876Z","iopub.status.idle":"2024-10-11T00:51:39.656444Z","shell.execute_reply.started":"2024-10-11T00:51:36.367834Z","shell.execute_reply":"2024-10-11T00:51:39.655625Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"datamodule1 = Datamodule1(batch_size=32, num_workers=0)\n\ndatamodule1.setup(stage=\"fit\")\n\nbatch1 = next(iter(datamodule1.train_dataloader()))\n\nfor i in batch1:\n\n    print(i.shape)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","torch.Size([32, 3, 32, 32])\n","torch.Size([32])\n"]}],"execution_count":10},{"cell_type":"markdown","source":"Напишем класс для организации обучения и добавим метрики:","metadata":{}},{"cell_type":"code","source":"from typing import Any\n\n\n\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\n\nimport torchmetrics.classification\n\n\n\n\n\ndef create_classification_metrics(\n\n    num_classes: int, prefix: str\n\n) -> torchmetrics.MetricCollection:\n\n    return torchmetrics.MetricCollection(\n\n        [\n\n            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n\n            #torchmetrics.classification.MulticlassAUROC(\n\n            #    num_classes=num_classes, average=\"macro\"\n\n            #),\n\n        ],\n\n        prefix=prefix,\n\n    )\n\n\n\n\n\nclass Lit(L.LightningModule):\n\n    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.model = model\n\n        self.learning_rate = learning_rate\n\n        self.train_metrics = create_classification_metrics(\n\n            num_classes=10, prefix=\"train_\"\n\n        )\n\n        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n\n\n\n    def training_step(\n\n        self, batch: tuple[Tensor, Tensor], batch_idx: int\n\n    ) -> STEP_OUTPUT: \n\n        x, y = batch\n\n        y_hat = self.model(x)\n\n        loss = F.cross_entropy(y_hat, y)\n\n        # loss теперь сохраняем только раз в эпоху\n\n        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n\n        # обновляем метрики и логируем раз в эпоху\n\n        self.train_metrics.update(y_hat, y)\n\n        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n\n        return loss\n\n\n\n    def validation_step(\n\n        self, batch: tuple[Tensor, Tensor], batch_idx: int\n\n    ) -> STEP_OUTPUT | None:\n\n        x, y = batch\n\n        y_hat = self.model(x)\n\n        loss = F.cross_entropy(y_hat, y)\n\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n\n        # обновляем метрики и логируем раз в эпоху\n\n        self.val_metrics.update(y_hat, y)\n\n        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n\n        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n\n       \n\n        return {\n\n            \"loss\": loss,\n\n            \"preds\": y_hat,\n\n        }\n\n\n\n    def configure_optimizers(self) -> dict[str, Any]:\n\n        optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.learning_rate, weight_decay=0.00001)\n\n        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n\n        return {\n\n            \"optimizer\": optimizer,\n\n            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n\n                optimizer, milestones=[5, 10, 15]\n\n            ),\n\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:46:34.164045Z","iopub.execute_input":"2024-10-11T00:46:34.164948Z","iopub.status.idle":"2024-10-11T00:46:34.177173Z","shell.execute_reply.started":"2024-10-11T00:46:34.164908Z","shell.execute_reply":"2024-10-11T00:46:34.176339Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"Создаем модель:","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.callbacks.model_summary import summarize\n\n\n\nmy_res_net = Lit(\n\n    model=MyResNet(ResidualBlock, n_classes = 10, hidden_channels=[16, 32, 64, 128, 128]), learning_rate=0.001\n\n)\n\nprint(summarize(my_res_net, max_depth=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:46:36.603761Z","iopub.execute_input":"2024-10-11T00:46:36.604118Z","iopub.status.idle":"2024-10-11T00:46:36.636416Z","shell.execute_reply.started":"2024-10-11T00:46:36.604087Z","shell.execute_reply":"2024-10-11T00:46:36.635576Z"}},"outputs":[{"name":"stdout","text":"  | Name                             | Type               | Params | Mode \n--------------------------------------------------------------------------------\n0 | model                            | MyResNet           | 615 K  | train\n1 | model.in_conv                    | Conv2d             | 448    | train\n2 | model.relu                       | ReLU               | 0      | train\n3 | model.features                   | Sequential         | 613 K  | train\n4 | model.maxpool                    | AdaptiveMaxPool2d  | 0      | train\n5 | model.fc                         | Linear             | 1.3 K  | train\n6 | train_metrics                    | MetricCollection   | 0      | train\n7 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n8 | val_metrics                      | MetricCollection   | 0      | train\n9 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n--------------------------------------------------------------------------------\n615 K     Trainable params\n0         Non-trainable params\n615 K     Total params\n2.462     Total estimated model params size (MB)\n42        Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Добавим callbacks для вывода потери:","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n\n\n\nfrom typing import cast\n\n\n\nfrom lightning.pytorch.callbacks import Callback\n\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\n\nfrom torchmetrics.classification.confusion_matrix import ConfusionMatrix\n\n\n\n\n\nclass MyPrintingCallback(Callback):\n\n    def on_validation_epoch_end(\n\n        self, trainer: L.Trainer, pl_module: L.LightningModule\n\n    ) -> None:\n\n        print(f'Accuracy val: {my_res_net.val_metrics.compute()}')\n\n\n\n\n\ncallbacks = [MyPrintingCallback(),\n\n             ModelCheckpoint(\n\n                    filename=\"{epoch}-{val_loss:.2f}\",\n\n                monitor=\"val_loss\",\n\n                mode=\"min\",\n\n        save_top_k=2,\n\n        save_last=True,\n\n    ),\n\n    EarlyStopping(\n\n        monitor=\"val_loss\",\n\n        mode=\"min\",\n\n        patience=3,\n\n    )]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:51:53.710914Z","iopub.execute_input":"2024-10-11T00:51:53.711906Z","iopub.status.idle":"2024-10-11T00:51:53.724897Z","shell.execute_reply.started":"2024-10-11T00:51:53.711857Z","shell.execute_reply":"2024-10-11T00:51:53.724069Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Напишем trainer и запустим обучение:","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.loggers import TensorBoardLogger\n\nfrom aim.pytorch_lightning import AimLogger\n\n# import os\n\n\n\n# Set the environment variable\n\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n\nu = \"Задание 2\"\n\nlogger2 = AimLogger(repo=\"logs\", experiment=str(u))\n\n\n\ntrainer2 = L.Trainer(\n\n    accelerator=\"auto\",\n\n    max_epochs=15,\n\n    logger=logger2,\n\n    callbacks=callbacks\n\n)\n\n\n\ntrainer2.fit(\n\n    model=my_res_net,\n\n    datamodule=datamodule1,\n\n)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name          | Type             | Params | Mode \n","-----------------------------------------------------------\n","0 | model         | MyResNet         | 615 K  | train\n","1 | train_metrics | MetricCollection | 0      | train\n","2 | val_metrics   | MetricCollection | 0      | train\n","-----------------------------------------------------------\n","615 K     Trainable params\n","0         Non-trainable params\n","615 K     Total params\n","2.462     Total estimated model params size (MB)\n","42        Modules in train mode\n","0         Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"963f9be2c1ea4ee999fe49d2a9c1a236","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.0625, device='cuda:0')}\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachaiugai/anaconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f926ff5cd8c470983d13d44db160b61","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"797c658d9e184ff698f613a045d4aafc","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.7322, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06aafb3074a94260bbcc9ad9d92fd2cd","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.7849, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18f442df6b6a40a3972485e422bdf298","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8086, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe8e4d8181ef4c139fdba4d0aa477fd5","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8002, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40e37f4bbf7a4796a6a28abd866d5bca","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8322, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6152267b86dc4614bf9497e261c51946","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8651, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aec42ee0cdd04688b27dc4631abcb7c8","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8650, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1beeec5ff1e74db1afddb6a165cd303e","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8627, device='cuda:0')}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"058d2bcfe6fe4fe2aaf5db4c0de8f17f","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8649, device='cuda:0')}\n"]}],"execution_count":14},{"cell_type":"code","source":"trainer2.validate(\n\n    model=my_res_net,\n\n    datamodule=datamodule1\n\n)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Force-releasing locks for Run '8a8e5ae767154efeb210e204'. Data corruption may occur if there is active process writing to Run '8a8e5ae767154efeb210e204'.\n"]},{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8a5659c5fa5418dafe3e21c6af825ed","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy val: {'val_MulticlassAccuracy': tensor(0.8649, device='cuda:0')}\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">  val_MulticlassAccuracy   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8648999929428101     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5106332898139954     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m val_MulticlassAccuracy  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8648999929428101    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5106332898139954    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'val_loss': 0.5106332898139954,\n","  'val_MulticlassAccuracy': 0.8648999929428101}]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"cell_type":"markdown","source":"#### Задание 3. Добавление аугментаций (1 балл + 2 балла за точность на валидации более 85%)\n\n# (я добавил из сразу в модули данных)\n\n\n\nДобавьте к обучающему датасету аугментации - случайные трансформации входных данных. Для этого можно использовать `torchvision.transforms` и `albumentations`.\n\n\n\nС `torchvision.transforms` совсем просто: вам нужно будет при создании `Datamodule` из практики по `lightning` указать вместо\n\n\n\n```python\n\ntransform = transforms.ToTensor()\n\n```\n\nкомпозицию трансформаций:\n\n\n\n```python\n\ntransform = transforms.Compose([\n\n    transforms.RandomHorizontalFlip(),  # случайное зеркальное отражение\n\n    ...\n\n    transforms.ToTensor(),\n\n])\n\n```","metadata":{}},{"cell_type":"markdown","source":"В пакете `albumentations` аугментаций значительно больше:\n\n\n\n![albumentations](https://albumentations.ai/assets/img/custom/top_image.jpg)","metadata":{}},{"cell_type":"markdown","source":"#### Задание 4. Использование предобученной модели (4 балла)\n\n\n\nТеперь мы научимся использовать модели, обученные на других задачах\n\n\n\nВаша задача: добиться 90% точности на тестовой выборке CIFAR-10. Постарайтесь уложиться модель с ~5 млн параметров","metadata":{}},{"cell_type":"markdown","source":"В `torchvision.models` есть много реализованных архитектур, размером которых можно удобно управлять. Например, ниже можно создать крошечную версию модели `MobileNetV2`:","metadata":{}},{"cell_type":"code","source":"from torchvision.models import MobileNetV2\n\n\n\nmobilenet = MobileNetV2(\n\n    num_classes=10,\n\n    width_mult=0.4,\n\n    inverted_residual_setting=[\n\n        # t, c, n, s\n\n        [1, 16, 1, 1],\n\n        [3, 24, 2, 2],\n\n        [3, 32, 3, 2],\n\n    ],\n\n    dropout=0.2,\n\n)\n\n\n\nsum([param.numel() for param in mobilenet.parameters()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T00:53:55.068330Z","iopub.execute_input":"2024-10-11T00:53:55.068808Z","iopub.status.idle":"2024-10-11T00:53:55.089758Z","shell.execute_reply.started":"2024-10-11T00:53:55.068748Z","shell.execute_reply":"2024-10-11T00:53:55.088834Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"46322"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"Но кроме архитектуры модели, мы также можем скачать веса, полученные при обучении на каком-то датасете. Например, для нашей задачи можно использовать предобучение на самом известном датасете для классификации изображений - ImageNet:","metadata":{}},{"cell_type":"code","source":"from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0\n\n\n\n# создаём EfficientNet с весами, полученными на ImageNet\n\nweights = EfficientNet_B0_Weights.IMAGENET1K_V1\n\nefficientnet = efficientnet_b0(weights=weights)\n\nsum([param.numel() for param in efficientnet.parameters()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:16.837852Z","iopub.execute_input":"2024-10-11T01:39:16.838640Z","iopub.status.idle":"2024-10-11T01:39:16.997693Z","shell.execute_reply.started":"2024-10-11T01:39:16.838598Z","shell.execute_reply":"2024-10-11T01:39:16.996717Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"5288548"},"metadata":{}}],"execution_count":71},{"cell_type":"markdown","source":"**Указание 1.** С использованием модели в исходном виде есть проблема: в ImageNet 1000 классов, а у нас только 10. Поэтому в предобученной модели нужно будет полностью заменить последний линейный слой, который даёт распределение вероятностей классов. Это можно сделать уже в готовом объекте модели, переназначив атрибут.\n\n\n\nПодсказка: в `efficientnet_b0` линейный слой находится в атрибуте `classifier` \n\n\n\n\n\n**Указание 2.** Все слои, кроме нескольких последних (может быть, только последнего) мы можем заморозить, то есть сделать значения параметров в них неизменными. Это позволит и сохранить способность модели выделять полезные низкоуровневые признаки (она научилась этому на ImageNet), и существенно ускорить дообучение.\n\n\n\n\n\nЧтобы заморозить параметры, нужно всего лишь отключить для них расчёт градиентов. Вернитесь к первой практике, чтобы вспомнить, как это можно сделать. Нам подойдёт самый простой способ с `.requires_grad`.\n\n\n\nПодсказка: в `efficientnet_b0` свёрточные слои находятся в атрибуте `features` ","metadata":{}},{"cell_type":"markdown","source":"**Указание 3.** Предобученные модели на ImageNet ожидают специальным образом трансформированные изображения:\n","metadata":{}},{"cell_type":"code","source":"weights.transforms()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:20.812023Z","iopub.execute_input":"2024-10-11T01:39:20.812804Z","iopub.status.idle":"2024-10-11T01:39:20.818676Z","shell.execute_reply.started":"2024-10-11T01:39:20.812753Z","shell.execute_reply":"2024-10-11T01:39:20.817718Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"Поэтому эти трансформации нужно будет передать в датамодуль (как мы делали с аугментациями).","metadata":{}},{"cell_type":"markdown","source":"ВАШ ХОД: Обучите модель и выведите результат метода validate на удачном чекпоинте","metadata":{}},{"cell_type":"markdown","source":"##### Модуль данных:","metadata":{}},{"cell_type":"code","source":"from typing import Callable\n\n\n\nimport lightning as L\n\nfrom lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n\nfrom PIL.Image import Image\n\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import datasets, transforms\n\n\n\n'''transform_to = transforms.Compose([\n\n            transforms.CenterCrop(224),\n\n            transforms.Resize(size=256,  interpolation=transforms.InterpolationMode.BICUBIC),\n\n            transforms.ToTensor(),\n\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n            #transforms.RandomHorizontalFlip(), \n\n\n\n    \n\n])'''\n\n\n\ntransform_to = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n\nclass Datamodule(L.LightningDataModule):\n\n    def __init__(\n\n        self,\n\n        batch_size: int,\n\n        transform: Callable[[Image], Tensor] = transform_to,\n\n        num_workers: int = 0,\n\n    ):\n\n        super().__init__()\n\n        self.batch_size = batch_size\n\n        self.transform = transform\n\n        self.num_workers = num_workers\n\n\n\n    def prepare_data(self) -> None:\n\n        # в этом методе можно сделать предварительную работу, например\n\n        # скачать данные, сделать тяжёлый препроцессинг\n\n        pass\n\n\n\n    def setup(self, stage: str) -> None:\n\n        # аргумент `stage` будет приходить из модуля обучения Trainer\n\n        # на стадии обучения (fit) нам нужны оба датасета\n\n        if stage == \"fit\":\n\n            self.train_dataset = torch.utils.data.ConcatDataset([datasets.CIFAR10(\n\n                \"data\",\n\n                train=True,\n\n                download=True,\n\n                transform=self.transform,\n\n            ), datasets.CIFAR10(\n\n                \"data\",\n\n                train=True,\n\n                download=True,\n\n                transform=transforms.Compose([self.transform,self.transform]),\n\n            )])\n\n            self.val_dataset = datasets.CIFAR10(\n\n                \"data\",\n\n                train=False,\n\n                download=True,\n\n                transform=transform_to,\n\n            )\n\n        # на стадии валидации (validate) - только тестовый\n\n        elif stage == \"validate\":\n\n            self.val_dataset = datasets.CIFAR10(\n\n                \"data\",\n\n                train=False,\n\n                download=True,\n\n                transform=transform_to,\n\n            )\n\n        else:\n\n            raise NotImplementedError\n\n        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n\n\n\n    def train_dataloader(self) -> TRAIN_DATALOADERS:\n\n        return DataLoader(\n\n            self.train_dataset,\n\n            batch_size=self.batch_size,\n\n            shuffle=True,\n\n            num_workers=self.num_workers,\n\n        )\n\n\n\n    def val_dataloader(self) -> EVAL_DATALOADERS:\n\n        return DataLoader(\n\n            self.val_dataset,\n\n            batch_size=self.batch_size,\n\n            shuffle=False,\n\n            num_workers=self.num_workers,\n\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:23.586769Z","iopub.execute_input":"2024-10-11T01:39:23.587168Z","iopub.status.idle":"2024-10-11T01:39:23.601227Z","shell.execute_reply.started":"2024-10-11T01:39:23.587130Z","shell.execute_reply":"2024-10-11T01:39:23.600214Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"datamodule = Datamodule(batch_size=32, num_workers=0)\n\ndatamodule.setup(stage=\"fit\")\n\nbatch = next(iter(datamodule.train_dataloader()))\n\nfor item in batch:\n\n    print(item.shape)\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:29.349331Z","iopub.execute_input":"2024-10-11T01:39:29.350427Z","iopub.status.idle":"2024-10-11T01:39:31.882239Z","shell.execute_reply.started":"2024-10-11T01:39:29.350366Z","shell.execute_reply":"2024-10-11T01:39:31.881215Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\ntorch.Size([32, 3, 224, 224])\ntorch.Size([32])\n","output_type":"stream"}],"execution_count":74},{"cell_type":"markdown","source":"Изменяем последний слой в архитектуре используемой модели:","metadata":{}},{"cell_type":"code","source":"efficientnet.classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:35.418191Z","iopub.execute_input":"2024-10-11T01:39:35.419098Z","iopub.status.idle":"2024-10-11T01:39:35.425108Z","shell.execute_reply.started":"2024-10-11T01:39:35.419057Z","shell.execute_reply":"2024-10-11T01:39:35.424192Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Dropout(p=0.2, inplace=True)\n  (1): Linear(in_features=1280, out_features=1000, bias=True)\n)"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"efficientnet.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True),\n\n                                        nn.Linear(in_features=1280, out_features=10, bias=True))\n\nefficientnet.features[8]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:38.350501Z","iopub.execute_input":"2024-10-11T01:39:38.351162Z","iopub.status.idle":"2024-10-11T01:39:38.359011Z","shell.execute_reply.started":"2024-10-11T01:39:38.351120Z","shell.execute_reply":"2024-10-11T01:39:38.358059Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"Conv2dNormActivation(\n  (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): SiLU(inplace=True)\n)"},"metadata":{}}],"execution_count":76},{"cell_type":"markdown","source":"Отключаем расчет градиентов во всех слоях кроме последнего:","metadata":{}},{"cell_type":"code","source":"for i in range(len(efficientnet.features) - 3):\n\n    for param in efficientnet.features[i].parameters():\n\n       param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:42.532247Z","iopub.execute_input":"2024-10-11T01:39:42.533138Z","iopub.status.idle":"2024-10-11T01:39:42.538833Z","shell.execute_reply.started":"2024-10-11T01:39:42.533096Z","shell.execute_reply":"2024-10-11T01:39:42.537847Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"from typing import Any\n\n\n\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\n\nimport torchmetrics.classification\n\n\n\n\n\ndef create_classification_metrics(\n\n    num_classes: int, prefix: str\n\n) -> torchmetrics.MetricCollection:\n\n    return torchmetrics.MetricCollection(\n\n        [\n\n            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n\n        ],\n\n        prefix=prefix,\n\n    )\n\n\n\n\n\nclass Lit(L.LightningModule):\n\n    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.model = model\n\n        self.learning_rate = learning_rate\n\n        self.train_metrics = create_classification_metrics(\n\n            num_classes=10, prefix=\"train_\"\n\n        )\n\n        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n\n\n\n    def training_step(\n\n        self, batch: tuple[Tensor, Tensor], batch_idx: int\n\n    ) -> STEP_OUTPUT: \n\n        x, y = batch\n\n        y_hat = self.model(x)\n\n        loss = F.cross_entropy(y_hat, y)\n\n        # loss теперь сохраняем только раз в эпоху\n\n        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n\n        # обновляем метрики и логируем раз в эпоху\n\n        self.train_metrics.update(y_hat, y)\n\n        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n\n        return loss\n\n\n\n    def validation_step(\n\n        self, batch: tuple[Tensor, Tensor], batch_idx: int\n\n    ) -> STEP_OUTPUT | None:\n\n        x, y = batch\n\n        y_hat = self.model(x)\n\n        loss = F.cross_entropy(y_hat, y)\n\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n\n        # обновляем метрики и логируем раз в эпоху\n\n        self.val_metrics.update(y_hat, y)\n\n        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n\n        # на этот раз вернём предсказания - будем их потом использовать, чтобы отрисовывать confusion matrix\n\n       \n\n        return {\n\n            \"loss\": loss,\n\n            \"preds\": y_hat,\n\n        }\n\n\n\n    def configure_optimizers(self) -> dict[str, Any]:\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=0.00001)\n\n        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n\n        return {\n\n            \"optimizer\": optimizer,\n\n            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n\n                optimizer, milestones=[5, 10, 15]\n\n            ),\n\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:46.158216Z","iopub.execute_input":"2024-10-11T01:39:46.158590Z","iopub.status.idle":"2024-10-11T01:39:46.174571Z","shell.execute_reply.started":"2024-10-11T01:39:46.158556Z","shell.execute_reply":"2024-10-11T01:39:46.173430Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"from lightning.pytorch.callbacks.model_summary import summarize\n\n\n\nlit_module = Lit(\n\n    model=efficientnet, learning_rate=0.001\n\n)\n\nprint(summarize(lit_module, max_depth=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:55.100211Z","iopub.execute_input":"2024-10-11T01:39:55.100901Z","iopub.status.idle":"2024-10-11T01:39:55.189359Z","shell.execute_reply.started":"2024-10-11T01:39:55.100859Z","shell.execute_reply":"2024-10-11T01:39:55.188437Z"}},"outputs":[{"name":"stdout","text":"  | Name                             | Type               | Params | Mode \n--------------------------------------------------------------------------------\n0 | model                            | EfficientNet       | 4.0 M  | train\n1 | model.features                   | Sequential         | 4.0 M  | train\n2 | model.avgpool                    | AdaptiveAvgPool2d  | 0      | train\n3 | model.classifier                 | Sequential         | 12.8 K | train\n4 | train_metrics                    | MetricCollection   | 0      | train\n5 | train_metrics.MulticlassAccuracy | MulticlassAccuracy | 0      | train\n6 | val_metrics                      | MetricCollection   | 0      | train\n7 | val_metrics.MulticlassAccuracy   | MulticlassAccuracy | 0      | train\n--------------------------------------------------------------------------------\n3.2 M     Trainable params\n851 K     Non-trainable params\n4.0 M     Total params\n16.081    Total estimated model params size (MB)\n341       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"Добавим callbacks для вывода потери:","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n\n\n\nfrom typing import cast\n\n\n\nfrom lightning.pytorch.callbacks import Callback\n\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\n\nfrom torchmetrics.classification.confusion_matrix import ConfusionMatrix\n\nclass MyPrintingCallback(Callback):\n\n    def on_validation_epoch_end(\n\n        self, trainer: L.Trainer, pl_module: L.LightningModule\n\n    ) -> None:\n\n        print(f'Accuracy val: {lit_module.val_metrics.compute()}')\n\ncallbacks = [MyPrintingCallback(),\n\n             ModelCheckpoint(\n\n                    filename=\"{epoch}-{val_loss:.2f}\",\n\n                monitor=\"val_loss\",\n\n                mode=\"min\",\n\n        save_top_k=2,\n\n        save_last=True,\n\n    ),\n\n    EarlyStopping(\n\n        monitor=\"val_loss\",\n\n        mode=\"min\",\n\n        patience=3,\n\n    )]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:39:59.398275Z","iopub.execute_input":"2024-10-11T01:39:59.398666Z","iopub.status.idle":"2024-10-11T01:39:59.406554Z","shell.execute_reply.started":"2024-10-11T01:39:59.398627Z","shell.execute_reply":"2024-10-11T01:39:59.405579Z"}},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":"Напишем trainer и запустим обучение:","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.loggers import TensorBoardLogger\n\nfrom aim.pytorch_lightning import AimLogger\n\n# import os\n\n\n\n# Set the environment variable\n\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n\nu = \"Задание 4\"\n\nlogger = AimLogger(repo=\"logs\", experiment=str(u))\n\n\n\ntrainer = L.Trainer(\n\n    accelerator=\"auto\",\n\n    max_epochs=3,\n\n    logger=logger,\n\n    callbacks=callbacks\n\n)\n\n\n\ntrainer.fit(\n\n    model=lit_module,\n\n    datamodule=datamodule,\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:40:02.725153Z","iopub.execute_input":"2024-10-11T01:40:02.726032Z","iopub.status.idle":"2024-10-11T01:58:12.708011Z","shell.execute_reply.started":"2024-10-11T01:40:02.725989Z","shell.execute_reply":"2024-10-11T01:58:12.706934Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"},{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name          | Type             | Params | Mode \n-----------------------------------------------------------\n0 | model         | EfficientNet     | 4.0 M  | train\n1 | train_metrics | MetricCollection | 0      | train\n2 | val_metrics   | MetricCollection | 0      | train\n-----------------------------------------------------------\n3.2 M     Trainable params\n851 K     Non-trainable params\n4.0 M     Total params\n16.081    Total estimated model params size (MB)\n341       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d5e8b93d9b4116a36e8cd6bb72661a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy val: {'val_MulticlassAccuracy': tensor(0.0625, device='cuda:0')}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fe965f769e4c8e88491c47a8aeb51a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3daf192c0441b198f7e5c577e8b24a"}},"metadata":{}},{"name":"stdout","text":"Accuracy val: {'val_MulticlassAccuracy': tensor(0.9039, device='cuda:0')}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"874756d8383a4c1ebf36b0b37386d094"}},"metadata":{}},{"name":"stdout","text":"Accuracy val: {'val_MulticlassAccuracy': tensor(0.9062, device='cuda:0')}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74d0ed5bf0546d7bd6fd0f1e8abdd6a"}},"metadata":{}},{"name":"stdout","text":"Accuracy val: {'val_MulticlassAccuracy': tensor(0.9253, device='cuda:0')}\n","output_type":"stream"},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=3` reached.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"trainer.validate(\n\n    model=lit_module,\n\n    datamodule=datamodule\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T01:59:08.354640Z","iopub.execute_input":"2024-10-11T01:59:08.355044Z","iopub.status.idle":"2024-10-11T01:59:34.232750Z","shell.execute_reply.started":"2024-10-11T01:59:08.355006Z","shell.execute_reply":"2024-10-11T01:59:34.231694Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d191abb02f44e58f26b7071135c4b4"}},"metadata":{}},{"name":"stdout","text":"Accuracy val: {'val_MulticlassAccuracy': tensor(0.9253, device='cuda:0')}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m val_MulticlassAccuracy  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9253000020980835    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24201813340187073   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">  val_MulticlassAccuracy   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9253000020980835     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.24201813340187073    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"[{'val_loss': 0.24201813340187073,\n  'val_MulticlassAccuracy': 0.9253000020980835}]"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}